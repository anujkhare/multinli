{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from typing import Any, Dict, List\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import tensorboardX\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path) -> pd.DataFrame:\n",
    "    with open(file_path, 'r') as infile:\n",
    "        data = infile.read().split('\\n')\n",
    "\n",
    "    data = list(map(json.loads, data[:-1]))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(len(df))\n",
    "    df = df.loc[df.gold_label != '-']\n",
    "    print(len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = 'data/multinli_1.0_train.jsonl'\n",
    "file_path_val = 'data/multinli_1.0_dev_matched.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n",
      "392702\n",
      "CPU times: user 6.61 s, sys: 1.22 s, total: 7.83 s\n",
      "Wall time: 7.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = load_data(file_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "9815\n",
      "CPU times: user 257 ms, sys: 8.5 ms, total: 266 ms\n",
      "Wall time: 262 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_val = load_data(file_path_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>31193n</td>\n",
       "      <td>31193</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>( ( Conceptually ( cream skimming ) ) ( ( has ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>( ( ( Product and ) geography ) ( ( are ( what...</td>\n",
       "      <td>(ROOT (S (NP (NN Product) (CC and) (NN geograp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>101457e</td>\n",
       "      <td>101457</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>( you ( ( know ( during ( ( ( the season ) and...</td>\n",
       "      <td>(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>( You ( ( ( ( lose ( the things ) ) ( to ( the...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>134793e</td>\n",
       "      <td>134793</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>( ( One ( of ( our number ) ) ) ( ( will ( ( (...</td>\n",
       "      <td>(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>( ( ( A member ) ( of ( my team ) ) ) ( ( will...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>37397e</td>\n",
       "      <td>37397</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...</td>\n",
       "      <td>(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>( ( This information ) ( ( belongs ( to them )...</td>\n",
       "      <td>(ROOT (S (NP (DT This) (NN information)) (VP (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>50563n</td>\n",
       "      <td>50563</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>( yeah ( i ( ( tell you ) ( what ( ( though ( ...</td>\n",
       "      <td>(ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>( ( The ( tennis shoes ) ) ( ( have ( ( a rang...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels       genre  gold_label   pairID promptID  \\\n",
       "0        [neutral]  government     neutral   31193n    31193   \n",
       "1     [entailment]   telephone  entailment  101457e   101457   \n",
       "2     [entailment]     fiction  entailment  134793e   134793   \n",
       "3     [entailment]     fiction  entailment   37397e    37397   \n",
       "4        [neutral]   telephone     neutral   50563n    50563   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  Conceptually cream skimming has two basic dime...   \n",
       "1  you know during the season and i guess at at y...   \n",
       "2  One of our number will carry out your instruct...   \n",
       "3  How do you know? All this is their information...   \n",
       "4  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( Conceptually ( cream skimming ) ) ( ( has ...   \n",
       "1  ( you ( ( know ( during ( ( ( the season ) and...   \n",
       "2  ( ( One ( of ( our number ) ) ) ( ( will ( ( (...   \n",
       "3  ( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...   \n",
       "4  ( yeah ( i ( ( tell you ) ( what ( ( though ( ...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...   \n",
       "1  (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...   \n",
       "2  (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...   \n",
       "3  (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...   \n",
       "4  (ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( ( Product and ) geography ) ( ( are ( what...   \n",
       "1  ( You ( ( ( ( lose ( the things ) ) ( to ( the...   \n",
       "2  ( ( ( A member ) ( of ( my team ) ) ) ( ( will...   \n",
       "3  ( ( This information ) ( ( belongs ( to them )...   \n",
       "4  ( ( The ( tennis shoes ) ) ( ( have ( ( a rang...   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (NN Product) (CC and) (NN geograp...  \n",
       "1  (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...  \n",
       "2  (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...  \n",
       "3  (ROOT (S (NP (DT This) (NN information)) (VP (...  \n",
       "4  (ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral, entailment, neutral, neutral, neutral]</td>\n",
       "      <td>slate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>63735n</td>\n",
       "      <td>63735</td>\n",
       "      <td>The new rights are nice enough</td>\n",
       "      <td>( ( The ( new rights ) ) ( are ( nice enough )...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (JJ new) (NNS rights)) (...</td>\n",
       "      <td>Everyone really likes the newest benefits</td>\n",
       "      <td>( Everyone ( really ( likes ( the ( newest ben...</td>\n",
       "      <td>(ROOT (S (NP (NN Everyone)) (VP (ADVP (RB real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
       "      <td>government</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>91383c</td>\n",
       "      <td>91383</td>\n",
       "      <td>This site includes a list of all award winners...</td>\n",
       "      <td>( ( This site ) ( ( includes ( ( ( ( a list ) ...</td>\n",
       "      <td>(ROOT (S (NP (DT This) (NN site)) (VP (VBZ inc...</td>\n",
       "      <td>The Government Executive articles housed on th...</td>\n",
       "      <td>( ( ( The ( Government ( Executive articles ) ...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT The) (NNP Government) (NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment, entailment, entailment, entailmen...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>755e</td>\n",
       "      <td>755</td>\n",
       "      <td>uh i don't know i i have mixed emotions about ...</td>\n",
       "      <td>( ( ( ( uh ( i ( ( do n't ) ( know ( ( i i ) (...</td>\n",
       "      <td>(ROOT (SINV (S (S (INTJ (UH uh)) (NP (FW i)) (...</td>\n",
       "      <td>I like him for the most part, but would still ...</td>\n",
       "      <td>( I ( ( ( ( ( ( like him ) ( for ( the ( most ...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VP (VBP like) (NP (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>78013c</td>\n",
       "      <td>78013</td>\n",
       "      <td>yeah i i think my favorite restaurant is alway...</td>\n",
       "      <td>( yeah ( ( i i ) ( think ( ( my ( favorite res...</td>\n",
       "      <td>(ROOT (S (VP (VB yeah) (NP (NP (FW i) (FW i)) ...</td>\n",
       "      <td>My favorite restaurants are always at least a ...</td>\n",
       "      <td>( ( My ( favorite restaurants ) ) ( ( ( ( are ...</td>\n",
       "      <td>(ROOT (S (NP (PRP$ My) (JJ favorite) (NNS rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>96377c</td>\n",
       "      <td>96377</td>\n",
       "      <td>i don't know um do you do a lot of camping</td>\n",
       "      <td>( i ( ( do n't ) ( know ( um ( do ( you ( do (...</td>\n",
       "      <td>(ROOT (S (NP (FW i)) (VP (VBP do) (RB n't) (VP...</td>\n",
       "      <td>I know exactly.</td>\n",
       "      <td>( I ( ( know exactly ) . ) )</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBP know) (ADVP (RB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    annotator_labels       genre  \\\n",
       "0   [neutral, entailment, neutral, neutral, neutral]       slate   \n",
       "1  [contradiction, contradiction, contradiction, ...  government   \n",
       "2  [entailment, entailment, entailment, entailmen...   telephone   \n",
       "3  [contradiction, contradiction, contradiction, ...   telephone   \n",
       "4  [contradiction, contradiction, contradiction, ...   telephone   \n",
       "\n",
       "      gold_label  pairID promptID  \\\n",
       "0        neutral  63735n    63735   \n",
       "1  contradiction  91383c    91383   \n",
       "2     entailment    755e      755   \n",
       "3  contradiction  78013c    78013   \n",
       "4  contradiction  96377c    96377   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0                     The new rights are nice enough   \n",
       "1  This site includes a list of all award winners...   \n",
       "2  uh i don't know i i have mixed emotions about ...   \n",
       "3  yeah i i think my favorite restaurant is alway...   \n",
       "4         i don't know um do you do a lot of camping   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( The ( new rights ) ) ( are ( nice enough )...   \n",
       "1  ( ( This site ) ( ( includes ( ( ( ( a list ) ...   \n",
       "2  ( ( ( ( uh ( i ( ( do n't ) ( know ( ( i i ) (...   \n",
       "3  ( yeah ( ( i i ) ( think ( ( my ( favorite res...   \n",
       "4  ( i ( ( do n't ) ( know ( um ( do ( you ( do (...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (DT The) (JJ new) (NNS rights)) (...   \n",
       "1  (ROOT (S (NP (DT This) (NN site)) (VP (VBZ inc...   \n",
       "2  (ROOT (SINV (S (S (INTJ (UH uh)) (NP (FW i)) (...   \n",
       "3  (ROOT (S (VP (VB yeah) (NP (NP (FW i) (FW i)) ...   \n",
       "4  (ROOT (S (NP (FW i)) (VP (VBP do) (RB n't) (VP...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0         Everyone really likes the newest benefits    \n",
       "1  The Government Executive articles housed on th...   \n",
       "2  I like him for the most part, but would still ...   \n",
       "3  My favorite restaurants are always at least a ...   \n",
       "4                                    I know exactly.   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( Everyone ( really ( likes ( the ( newest ben...   \n",
       "1  ( ( ( The ( Government ( Executive articles ) ...   \n",
       "2  ( I ( ( ( ( ( ( like him ) ( for ( the ( most ...   \n",
       "3  ( ( My ( favorite restaurants ) ) ( ( ( ( are ...   \n",
       "4                       ( I ( ( know exactly ) . ) )   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (NN Everyone)) (VP (ADVP (RB real...  \n",
       "1  (ROOT (S (NP (NP (DT The) (NNP Government) (NN...  \n",
       "2  (ROOT (S (NP (PRP I)) (VP (VP (VBP like) (NP (...  \n",
       "3  (ROOT (S (NP (PRP$ My) (JJ favorite) (NNS rest...  \n",
       "4  (ROOT (S (NP (PRP I)) (VP (VBP know) (ADVP (RB...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my walkman broke so i'm upset now i just have ...</td>\n",
       "      <td>I'm upset that my walkman broke and now I have...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>But a few Christian mosaics survive above the ...</td>\n",
       "      <td>Most of the Christian mosaics were destroyed b...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Read  for Slate 's take on Jackson's findings.)</td>\n",
       "      <td>Slate had an opinion on Jackson's findings.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gays and lesbians.</td>\n",
       "      <td>Heterosexuals.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>At the end of Rue des Francs-Bourgeois is what...</td>\n",
       "      <td>Place des Vosges is constructed entirely of gr...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  Conceptually cream skimming has two basic dime...   \n",
       "1  you know during the season and i guess at at y...   \n",
       "2  One of our number will carry out your instruct...   \n",
       "3  How do you know? All this is their information...   \n",
       "4  yeah i tell you what though if you go price so...   \n",
       "5  my walkman broke so i'm upset now i just have ...   \n",
       "6  But a few Christian mosaics survive above the ...   \n",
       "7   (Read  for Slate 's take on Jackson's findings.)   \n",
       "8                                 Gays and lesbians.   \n",
       "9  At the end of Rue des Francs-Bourgeois is what...   \n",
       "\n",
       "                                           sentence2     gold_label  \n",
       "0  Product and geography are what make cream skim...        neutral  \n",
       "1  You lose the things to the following level if ...     entailment  \n",
       "2  A member of my team will execute your orders w...     entailment  \n",
       "3                  This information belongs to them.     entailment  \n",
       "4           The tennis shoes have a range of prices.        neutral  \n",
       "5  I'm upset that my walkman broke and now I have...     entailment  \n",
       "6  Most of the Christian mosaics were destroyed b...        neutral  \n",
       "7        Slate had an opinion on Jackson's findings.     entailment  \n",
       "8                                     Heterosexuals.  contradiction  \n",
       "9  Place des Vosges is constructed entirely of gr...  contradiction  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)[['sentence1', 'sentence2', 'gold_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([207.,  16.,  13.,   8.,   7.,   9.,   7.,   8.,  12.,  14.]),\n",
       " array([1.0000e+00, 7.4250e+02, 1.4840e+03, 2.2255e+03, 2.9670e+03,\n",
       "        3.7085e+03, 4.4500e+03, 5.1915e+03, 5.9330e+03, 6.6745e+03,\n",
       "        7.4160e+03]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_train.sentence1.apply(len).value_counts())\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_train.sentence2.apply(len).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors(file_path: str) -> Dict:\n",
    "    with open(file_path, 'r') as infile:\n",
    "        data_glove = infile.read().split('\\n')\n",
    "\n",
    "    data_glove = map(lambda x: x.split(), data_glove)  # Split the words\n",
    "\n",
    "    glove = {\n",
    "        line[0]: np.array(line[1:], dtype=np.float32)\n",
    "        for line in data_glove\n",
    "        if len(line) == 301\n",
    "    }\n",
    "\n",
    "    return glove\n",
    "\n",
    "\n",
    "class MNLIDataset(data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, word_vectors: Dict) -> None:\n",
    "        df['label_id'] = df.gold_label.map(lbl_to_id)\n",
    "        df['sentence1'] = df['sentence1'].apply(lambda x: x.strip(string.punctuation))\n",
    "        df['sentence2'] = df['sentence2'].apply(lambda x: x.strip(string.punctuation))\n",
    "\n",
    "        self.df = df\n",
    "        self.word_vectors = word_vectors\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _sentence_to_vec(self, sentence: str) -> np.ndarray:\n",
    "        # FIXME\n",
    "        vectors = []\n",
    "        final_sentence = []\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in self.word_vectors:\n",
    "                vectors.append(np.zeros(300, dtype=np.float32))\n",
    "                continue\n",
    "\n",
    "            final_sentence.append(word)\n",
    "            vectors.append(self.word_vectors[word])\n",
    "\n",
    "#         vectors = np.vstack(vectors)\n",
    "        vectors = np.array(vectors)\n",
    "        final_sentence = ' '.join(final_sentence)\n",
    "        return vectors, final_sentence\n",
    "        \n",
    "    def _preprocess(self, record):\n",
    "        # Convert sentences to word vectors, return list of \n",
    "        v1, fs1 = self._sentence_to_vec(record['sentence1'])\n",
    "        v2, fs2 = self._sentence_to_vec(record['sentence2'])\n",
    "        return {\n",
    "            'sentence1': v1,\n",
    "            'sentence2': v2,\n",
    "            'label': record['label_id'],\n",
    "            'final_sentence1': fs1,\n",
    "            'final_sentence2': fs2,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self._preprocess(self.df.iloc[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_lbl = ['neutral', 'entailment', 'contradiction']\n",
    "lbl_to_id = {\n",
    "    lbl: ix\n",
    "    for ix, lbl in enumerate(id_to_lbl)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 µs, sys: 17 µs, total: 56 µs\n",
      "Wall time: 49.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pickle_file = 'models/glove.pickle'\n",
    "if not os.path.exists(pickle_file):\n",
    "    glove = load_word_vectors('models/glove.840B.300d.txt')  # FIXME: There shold be 2196017 words\n",
    "    print(len(glove))\n",
    "\n",
    "    with open(pickle_file, 'wb') as outfile:\n",
    "        pickle.dump(glove, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.48 s, sys: 2.9 s, total: 9.38 s\n",
      "Wall time: 9.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(pickle_file, 'rb') as infile:\n",
    "    glove = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataloader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_train = MNLIDataset(df_train, word_vectors=glove)\n",
    "dataloader_train = data.DataLoader(dataset=dataset_train, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_val = MNLIDataset(df_val, word_vectors=glove)\n",
    "dataloader_val = data.DataLoader(dataset=dataset_val, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702 392702\n",
      "9815 9815\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train), len(dataloader_train))\n",
    "print(len(dataset_val), len(dataloader_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contradiction    130903\n",
       "neutral          130900\n",
       "entailment       130899\n",
       "Name: gold_label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have balanced classes\n",
    "df_train.gold_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entailment       3479\n",
       "contradiction    3213\n",
       "neutral          3123\n",
       "Name: gold_label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have balanced classes\n",
    "df_val.gold_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vector_dim=300, n_out=3) -> None:\n",
    "        super().__init__()\n",
    "        in_size = vector_dim * 4\n",
    "        \n",
    "        self.feature_extractor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_size, 1024),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(1024, 1024),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(1024, 1024),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(1024, n_out)\n",
    "        self.vector_dim = vector_dim\n",
    "\n",
    "    def forward(self, data):\n",
    "        vectors1, vectors2 = data\n",
    "#         print(vectors1.shape, vectors2.shape)\n",
    "        s1, s2 = torch.sum(vectors1, dim=1), torch.sum(vectors2, dim=1)\n",
    "#         print(s1.shape, s2.shape)\n",
    "        \n",
    "        features = torch.cat([s1, s2, s1-s2, s1*s2], dim=1)\n",
    "        features = self.feature_extractor(features)\n",
    "#         print(features.shape)\n",
    "        pred = self.classifier(features)\n",
    "        return F.log_softmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vector_dim=300, proj_dim=300, num_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.projection = torch.nn.Linear(vector_dim, proj_dim)\n",
    "        self.lstm = torch.nn.LSTM(proj_dim, proj_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(proj_dim * num_layers * 2 * 4, 2048),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.Linear(2048, 3),\n",
    "        )\n",
    "    \n",
    "    def _sentence_features(self, vectors):\n",
    "        proj = self.projection(vectors)\n",
    "        _, (hT, _) = self.lstm(vectors)\n",
    "\n",
    "        sentence_vec = hT.transpose(0, 1)  # get batch first: B*num_layers*num_directions*300\n",
    "        sentence_vec = sentence_vec.contiguous().view(sentence_vec.shape[0], -1)\n",
    "        return sentence_vec\n",
    "\n",
    "    def forward(self, data):\n",
    "        vectors1, vectors2 = data\n",
    "\n",
    "        sentence_vec1 = self._sentence_features(vectors1)\n",
    "        sentence_vec2 = self._sentence_features(vectors2)\n",
    "        \n",
    "        features = torch.cat([sentence_vec1, sentence_vec1, sentence_vec1 - sentence_vec2, sentence_vec1 * sentence_vec2],\n",
    "                             dim=1)\n",
    "        pred = self.classifier(features)\n",
    "        return F.log_softmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lstm():\n",
    "    bsz = 3\n",
    "    seq1 = torch.from_numpy(np.random.rand(bsz, 20, 300,).astype(np.float32))\n",
    "    seq2 = torch.from_numpy(np.random.rand(bsz, 24, 300,).astype(np.float32))\n",
    "\n",
    "    model = LSTM()\n",
    "\n",
    "    outputs = model((seq1, seq2))\n",
    "    assert outputs.shape == (bsz, 3)\n",
    "\n",
    "test_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs(batch, device=0):\n",
    "    inputs = (Variable(batch['sentence1']).cuda(device=DEVICE),\n",
    "              Variable(batch['sentence2']).cuda(device=DEVICE))\n",
    "    label = Variable(batch['label']).cuda(device=DEVICE)\n",
    "\n",
    "    return inputs, label\n",
    "\n",
    "def evaluate(model, dataloader, loss_func, device, n_batches=10):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train(False)\n",
    "\n",
    "    for ix, batch in enumerate(dataloader):\n",
    "        if ix >= n_batches:\n",
    "            break\n",
    "        \n",
    "        inputs, label = prep_inputs(batch)\n",
    "        \n",
    "        predicted = model(inputs)\n",
    "\n",
    "        loss += loss_func(predicted, label)\n",
    "        prediction = torch.argmax(predicted, dim=1)\n",
    "\n",
    "        # TODO: get the confusion here\n",
    "        accuracy += (torch.sum(prediction == label).data.cpu().numpy() / len(prediction))\n",
    "\n",
    "    accuracy /= n_batches\n",
    "    loss /= n_batches\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 8\n",
    "val_every = 1000\n",
    "\n",
    "DEVICE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CBOW().cuda(device=DEVICE)\n",
    "model = LSTM().cuda(device=DEVICE)\n",
    "\n",
    "loss_func = torch.nn.NLLLoss().cuda(device=DEVICE)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = 'lstm-1.3'\n",
    "model_dir = 'models/{}'.format(model_str)\n",
    "log_dir = 'logs/{}'.format(model_str)\n",
    "\n",
    "os.makedirs(model_dir)\n",
    "writer = tensorboardX.SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10654847\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad: n_params += np.prod(param.size())\n",
    "\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_init = [p.clone() for p in list(model.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_params(params1, params2):\n",
    "    for ix, (param1, param2) in enumerate(zip(params1[::-1], params2[::-1])):\n",
    "        assert not np.all((param1 - param2).data.cpu().numpy() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, 0.25%: train loss: 0.9429846405982971, validation loss: 1.1070126295089722, validation acc: 0.315\n",
      "Epoch: 0, 0.51%: train loss: 1.0366706848144531, validation loss: 1.0879102945327759, validation acc: 0.365\n",
      "Epoch: 0, 0.76%: train loss: 1.1959459781646729, validation loss: 1.0961562395095825, validation acc: 0.38\n",
      "Epoch: 0, 1.02%: train loss: 0.9823434948921204, validation loss: 1.104907512664795, validation acc: 0.32\n",
      "Epoch: 0, 1.27%: train loss: 1.0809249877929688, validation loss: 1.0903748273849487, validation acc: 0.37\n",
      "Epoch: 0, 1.53%: train loss: 1.0018305778503418, validation loss: 1.066785454750061, validation acc: 0.4\n",
      "Epoch: 0, 1.78%: train loss: 1.262559413909912, validation loss: 1.0909974575042725, validation acc: 0.38\n",
      "Epoch: 0, 2.04%: train loss: 1.2582809925079346, validation loss: 1.0709049701690674, validation acc: 0.395\n",
      "Epoch: 0, 2.29%: train loss: 0.8611787557601929, validation loss: 1.0536106824874878, validation acc: 0.435\n",
      "Epoch: 0, 2.55%: train loss: 1.2455990314483643, validation loss: 1.0448591709136963, validation acc: 0.395\n",
      "Epoch: 0, 2.80%: train loss: 1.3465580940246582, validation loss: 1.063658595085144, validation acc: 0.445\n",
      "Epoch: 0, 3.06%: train loss: 1.0258160829544067, validation loss: 1.074452519416809, validation acc: 0.415\n",
      "Epoch: 0, 3.31%: train loss: 1.368190884590149, validation loss: 1.0453051328659058, validation acc: 0.455\n",
      "Epoch: 0, 3.57%: train loss: 1.9519753456115723, validation loss: 1.0413986444473267, validation acc: 0.445\n",
      "Epoch: 0, 3.82%: train loss: 0.9426094889640808, validation loss: 1.0123355388641357, validation acc: 0.465\n",
      "Epoch: 0, 4.07%: train loss: 0.8485950231552124, validation loss: 1.0648436546325684, validation acc: 0.425\n",
      "Epoch: 0, 4.33%: train loss: 1.0461113452911377, validation loss: 0.9759005308151245, validation acc: 0.545\n",
      "Epoch: 0, 4.58%: train loss: 0.8036129474639893, validation loss: 1.0327117443084717, validation acc: 0.435\n",
      "Epoch: 0, 4.84%: train loss: 0.7330190539360046, validation loss: 1.0597219467163086, validation acc: 0.47\n",
      "Epoch: 0, 5.09%: train loss: 0.436393141746521, validation loss: 1.0461065769195557, validation acc: 0.48\n",
      "Epoch: 0, 5.35%: train loss: 1.0140125751495361, validation loss: 1.0686492919921875, validation acc: 0.465\n",
      "Epoch: 0, 5.60%: train loss: 1.1520113945007324, validation loss: 0.9848586320877075, validation acc: 0.51\n",
      "Epoch: 0, 5.86%: train loss: 0.902224063873291, validation loss: 1.069313406944275, validation acc: 0.41\n",
      "Epoch: 0, 6.11%: train loss: 0.6628598570823669, validation loss: 1.0235364437103271, validation acc: 0.505\n",
      "Epoch: 0, 6.37%: train loss: 0.4369915723800659, validation loss: 1.0228919982910156, validation acc: 0.475\n",
      "Epoch: 0, 6.62%: train loss: 0.6341556310653687, validation loss: 1.0068012475967407, validation acc: 0.515\n",
      "Epoch: 0, 6.88%: train loss: 0.6650277972221375, validation loss: 1.039401888847351, validation acc: 0.475\n",
      "Epoch: 0, 7.13%: train loss: 0.4705616235733032, validation loss: 1.1036080121994019, validation acc: 0.4\n",
      "Epoch: 0, 7.38%: train loss: 1.5887696743011475, validation loss: 1.0519258975982666, validation acc: 0.455\n",
      "Epoch: 0, 7.64%: train loss: 0.8024896383285522, validation loss: 1.027069330215454, validation acc: 0.48\n",
      "Epoch: 0, 7.89%: train loss: 1.0774798393249512, validation loss: 1.0072048902511597, validation acc: 0.48\n",
      "Epoch: 0, 8.15%: train loss: 0.704394519329071, validation loss: 1.034443974494934, validation acc: 0.48\n",
      "Epoch: 0, 8.40%: train loss: 0.19110798835754395, validation loss: 1.013624668121338, validation acc: 0.46\n",
      "Epoch: 0, 8.66%: train loss: 0.28339529037475586, validation loss: 1.0205119848251343, validation acc: 0.455\n",
      "Epoch: 0, 8.91%: train loss: 0.9440194368362427, validation loss: 1.0021986961364746, validation acc: 0.515\n",
      "Epoch: 0, 9.17%: train loss: 0.6907203793525696, validation loss: 1.041817545890808, validation acc: 0.475\n",
      "Epoch: 0, 9.42%: train loss: 0.6854559183120728, validation loss: 1.0446563959121704, validation acc: 0.455\n",
      "Epoch: 0, 9.68%: train loss: 0.6992690563201904, validation loss: 1.0262888669967651, validation acc: 0.495\n",
      "Epoch: 0, 9.93%: train loss: 0.99960857629776, validation loss: 0.9806545972824097, validation acc: 0.535\n",
      "Epoch: 0, 10.19%: train loss: 1.0359289646148682, validation loss: 1.016433835029602, validation acc: 0.48\n",
      "Epoch: 0, 10.44%: train loss: 1.4035910367965698, validation loss: 0.975640594959259, validation acc: 0.52\n",
      "Epoch: 0, 10.70%: train loss: 0.9520294070243835, validation loss: 1.0230810642242432, validation acc: 0.475\n",
      "Epoch: 0, 10.95%: train loss: 0.9113984107971191, validation loss: 1.0175912380218506, validation acc: 0.505\n",
      "Epoch: 0, 11.20%: train loss: 1.2665632963180542, validation loss: 0.9951808452606201, validation acc: 0.54\n",
      "Epoch: 0, 11.46%: train loss: 0.8431158065795898, validation loss: 1.0540292263031006, validation acc: 0.405\n",
      "Epoch: 0, 11.71%: train loss: 1.2298873662948608, validation loss: 1.0344812870025635, validation acc: 0.46\n",
      "Epoch: 0, 11.97%: train loss: 1.4670157432556152, validation loss: 1.005401849746704, validation acc: 0.515\n",
      "Epoch: 0, 12.22%: train loss: 0.36881494522094727, validation loss: 1.0322755575180054, validation acc: 0.49\n",
      "Epoch: 0, 12.48%: train loss: 0.5428427457809448, validation loss: 1.0519146919250488, validation acc: 0.455\n",
      "Epoch: 0, 12.73%: train loss: 0.8262580633163452, validation loss: 1.0610337257385254, validation acc: 0.49\n",
      "Epoch: 0, 12.99%: train loss: 0.8267778754234314, validation loss: 1.034527063369751, validation acc: 0.52\n",
      "Epoch: 0, 13.24%: train loss: 0.5494644045829773, validation loss: 0.969472348690033, validation acc: 0.565\n",
      "Epoch: 0, 13.50%: train loss: 1.0870044231414795, validation loss: 1.0702401399612427, validation acc: 0.465\n",
      "Epoch: 0, 13.75%: train loss: 1.0540802478790283, validation loss: 1.0169978141784668, validation acc: 0.465\n",
      "Epoch: 0, 14.01%: train loss: 0.7316954731941223, validation loss: 1.036112666130066, validation acc: 0.46\n",
      "Epoch: 0, 14.26%: train loss: 0.7178984880447388, validation loss: 1.0117837190628052, validation acc: 0.515\n",
      "Epoch: 0, 14.51%: train loss: 0.9748595356941223, validation loss: 1.0227044820785522, validation acc: 0.5\n",
      "Epoch: 0, 14.77%: train loss: 1.0009174346923828, validation loss: 1.062777042388916, validation acc: 0.48\n",
      "Epoch: 0, 15.02%: train loss: 0.7094670534133911, validation loss: 0.9814576506614685, validation acc: 0.54\n",
      "Epoch: 0, 15.28%: train loss: 1.1288050413131714, validation loss: 0.985819935798645, validation acc: 0.505\n",
      "Epoch: 0, 15.53%: train loss: 0.452822208404541, validation loss: 0.9459942579269409, validation acc: 0.58\n",
      "Epoch: 0, 15.79%: train loss: 0.3185734748840332, validation loss: 1.0211163759231567, validation acc: 0.515\n",
      "Epoch: 0, 16.04%: train loss: 1.6699678897857666, validation loss: 1.0223472118377686, validation acc: 0.495\n",
      "Epoch: 0, 16.30%: train loss: 1.797796368598938, validation loss: 0.986181914806366, validation acc: 0.545\n",
      "Epoch: 0, 16.55%: train loss: 0.8834812641143799, validation loss: 1.0002102851867676, validation acc: 0.52\n",
      "Epoch: 0, 16.81%: train loss: 0.5763959884643555, validation loss: 0.9261214733123779, validation acc: 0.525\n",
      "Epoch: 0, 17.06%: train loss: 0.7109513878822327, validation loss: 0.9781017899513245, validation acc: 0.535\n",
      "Epoch: 0, 17.32%: train loss: 1.6559253931045532, validation loss: 0.9737172722816467, validation acc: 0.525\n",
      "Epoch: 0, 17.57%: train loss: 1.1337546110153198, validation loss: 0.9994140267372131, validation acc: 0.53\n",
      "Epoch: 0, 17.83%: train loss: 0.3113105297088623, validation loss: 0.978217363357544, validation acc: 0.54\n",
      "Epoch: 0, 18.08%: train loss: 1.7430182695388794, validation loss: 0.9822139739990234, validation acc: 0.49\n",
      "Epoch: 0, 18.33%: train loss: 0.6197665333747864, validation loss: 0.9953280687332153, validation acc: 0.515\n",
      "Epoch: 0, 18.59%: train loss: 1.2622380256652832, validation loss: 0.9624750018119812, validation acc: 0.61\n",
      "Epoch: 0, 18.84%: train loss: 0.3602721691131592, validation loss: 0.9940447211265564, validation acc: 0.535\n",
      "Epoch: 0, 19.10%: train loss: 0.9462528228759766, validation loss: 0.9948094487190247, validation acc: 0.545\n",
      "Epoch: 0, 19.35%: train loss: 0.6639076471328735, validation loss: 1.0219590663909912, validation acc: 0.515\n",
      "Epoch: 0, 19.61%: train loss: 0.6610584855079651, validation loss: 0.9545832872390747, validation acc: 0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, 19.86%: train loss: 1.3661848306655884, validation loss: 0.9893593192100525, validation acc: 0.515\n",
      "Epoch: 0, 20.12%: train loss: 1.4266242980957031, validation loss: 0.9912987947463989, validation acc: 0.52\n",
      "Epoch: 0, 20.37%: train loss: 0.37091517448425293, validation loss: 1.0075480937957764, validation acc: 0.51\n",
      "Epoch: 0, 20.63%: train loss: 1.3552892208099365, validation loss: 0.9607691764831543, validation acc: 0.555\n",
      "Epoch: 0, 20.88%: train loss: 0.7122458815574646, validation loss: 1.0491447448730469, validation acc: 0.455\n",
      "Epoch: 0, 21.14%: train loss: 0.6092854738235474, validation loss: 0.988633930683136, validation acc: 0.52\n",
      "Epoch: 0, 23.17%: train loss: 1.4049009084701538, validation loss: 0.9959254860877991, validation acc: 0.53\n",
      "Epoch: 0, 23.43%: train loss: 1.9361369609832764, validation loss: 0.9954379200935364, validation acc: 0.52\n",
      "Epoch: 0, 23.68%: train loss: 1.049988865852356, validation loss: 0.9735998511314392, validation acc: 0.52\n",
      "Epoch: 0, 23.94%: train loss: 0.8783937692642212, validation loss: 1.027126431465149, validation acc: 0.505\n",
      "Epoch: 0, 24.19%: train loss: 1.4343338012695312, validation loss: 1.0180156230926514, validation acc: 0.485\n",
      "Epoch: 0, 24.45%: train loss: 0.7710402011871338, validation loss: 0.9918385148048401, validation acc: 0.495\n",
      "Epoch: 0, 24.70%: train loss: 0.6034597754478455, validation loss: 1.0156279802322388, validation acc: 0.495\n",
      "Epoch: 0, 24.96%: train loss: 0.37912845611572266, validation loss: 0.971379280090332, validation acc: 0.545\n",
      "Epoch: 0, 25.21%: train loss: 0.40452080965042114, validation loss: 1.0310577154159546, validation acc: 0.485\n",
      "Epoch: 0, 25.46%: train loss: 0.8063833117485046, validation loss: 0.9461151957511902, validation acc: 0.57\n",
      "Epoch: 0, 25.72%: train loss: 0.5013318657875061, validation loss: 0.9731568098068237, validation acc: 0.555\n",
      "Epoch: 0, 25.97%: train loss: 0.8793197870254517, validation loss: 0.976042628288269, validation acc: 0.525\n",
      "Epoch: 0, 26.23%: train loss: 0.25021541118621826, validation loss: 0.9824713468551636, validation acc: 0.475\n",
      "Epoch: 0, 26.48%: train loss: 0.7614555358886719, validation loss: 1.0206239223480225, validation acc: 0.52\n",
      "Epoch: 0, 26.74%: train loss: 0.9307942390441895, validation loss: 0.9878178834915161, validation acc: 0.515\n",
      "Epoch: 0, 26.99%: train loss: 0.5310714244842529, validation loss: 1.0114097595214844, validation acc: 0.51\n",
      "Epoch: 0, 27.25%: train loss: 0.4049602150917053, validation loss: 0.9793895483016968, validation acc: 0.535\n",
      "Epoch: 0, 27.50%: train loss: 0.37168169021606445, validation loss: 0.9666430950164795, validation acc: 0.505\n",
      "Epoch: 0, 27.76%: train loss: 1.44133722782135, validation loss: 1.0120505094528198, validation acc: 0.55\n",
      "Epoch: 0, 28.01%: train loss: 0.5484342575073242, validation loss: 0.9752185344696045, validation acc: 0.495\n",
      "Epoch: 0, 28.27%: train loss: 0.7635565996170044, validation loss: 0.9987550973892212, validation acc: 0.515\n",
      "Epoch: 0, 28.52%: train loss: 0.7715762257575989, validation loss: 0.9808390736579895, validation acc: 0.525\n",
      "Epoch: 0, 28.77%: train loss: 0.4559441804885864, validation loss: 1.009138584136963, validation acc: 0.52\n",
      "Epoch: 0, 29.03%: train loss: 1.0391701459884644, validation loss: 0.9732282757759094, validation acc: 0.525\n",
      "Epoch: 0, 29.28%: train loss: 0.9354453086853027, validation loss: 0.9643281102180481, validation acc: 0.57\n",
      "Epoch: 0, 29.54%: train loss: 1.043143391609192, validation loss: 0.9749162793159485, validation acc: 0.54\n",
      "Epoch: 0, 29.79%: train loss: 0.8805893063545227, validation loss: 0.9324864745140076, validation acc: 0.58\n",
      "Epoch: 0, 30.05%: train loss: 1.3587380647659302, validation loss: 0.9993175864219666, validation acc: 0.485\n",
      "Epoch: 0, 30.30%: train loss: 1.1514452695846558, validation loss: 0.9707781076431274, validation acc: 0.545\n",
      "Epoch: 0, 30.56%: train loss: 0.8367548584938049, validation loss: 0.9422144889831543, validation acc: 0.53\n",
      "Epoch: 0, 30.81%: train loss: 0.7604403495788574, validation loss: 0.9831518530845642, validation acc: 0.51\n",
      "Epoch: 0, 31.07%: train loss: 1.3304592370986938, validation loss: 1.0186816453933716, validation acc: 0.485\n",
      "Epoch: 0, 31.32%: train loss: 0.7340751886367798, validation loss: 1.0092298984527588, validation acc: 0.515\n",
      "Epoch: 0, 31.58%: train loss: 0.9604898691177368, validation loss: 0.9461055397987366, validation acc: 0.585\n",
      "Epoch: 0, 31.83%: train loss: 1.1426194906234741, validation loss: 1.0066614151000977, validation acc: 0.475\n",
      "Epoch: 0, 32.09%: train loss: 1.332433819770813, validation loss: 0.9710411429405212, validation acc: 0.55\n",
      "Epoch: 0, 32.34%: train loss: 0.559980034828186, validation loss: 0.994536817073822, validation acc: 0.495\n",
      "Epoch: 0, 32.59%: train loss: 1.18159818649292, validation loss: 0.9708088636398315, validation acc: 0.53\n",
      "Epoch: 0, 32.85%: train loss: 0.4221037030220032, validation loss: 0.974027693271637, validation acc: 0.5\n",
      "Epoch: 0, 33.10%: train loss: 1.7391690015792847, validation loss: 0.995337963104248, validation acc: 0.535\n",
      "Epoch: 0, 33.36%: train loss: 1.0986676216125488, validation loss: 0.9591622948646545, validation acc: 0.545\n",
      "Epoch: 0, 33.61%: train loss: 1.0087742805480957, validation loss: 1.0289889574050903, validation acc: 0.485\n",
      "Epoch: 0, 33.87%: train loss: 0.4997316002845764, validation loss: 0.9879825115203857, validation acc: 0.49\n",
      "Epoch: 0, 34.12%: train loss: 0.9634652137756348, validation loss: 0.9949768781661987, validation acc: 0.495\n",
      "Epoch: 0, 34.38%: train loss: 0.7315280437469482, validation loss: 0.9551932215690613, validation acc: 0.545\n",
      "Epoch: 0, 34.63%: train loss: 0.6896324157714844, validation loss: 1.0273064374923706, validation acc: 0.485\n",
      "Epoch: 0, 34.89%: train loss: 0.3155020475387573, validation loss: 1.0304815769195557, validation acc: 0.47\n",
      "Epoch: 0, 35.14%: train loss: 0.17984938621520996, validation loss: 0.9897158741950989, validation acc: 0.51\n",
      "Epoch: 0, 35.40%: train loss: 0.7970889806747437, validation loss: 0.9732847213745117, validation acc: 0.53\n",
      "Epoch: 0, 35.65%: train loss: 1.3021109104156494, validation loss: 1.0521702766418457, validation acc: 0.48\n",
      "Epoch: 0, 35.91%: train loss: 0.4624323844909668, validation loss: 0.9480934143066406, validation acc: 0.49\n",
      "Epoch: 0, 36.16%: train loss: 0.5309647917747498, validation loss: 0.9679979681968689, validation acc: 0.55\n",
      "Epoch: 0, 36.41%: train loss: 1.3348777294158936, validation loss: 0.9651710391044617, validation acc: 0.53\n",
      "Epoch: 0, 36.67%: train loss: 1.9424433708190918, validation loss: 0.9407649636268616, validation acc: 0.58\n",
      "Epoch: 0, 36.92%: train loss: 0.9986662268638611, validation loss: 1.0465240478515625, validation acc: 0.455\n",
      "Epoch: 0, 37.18%: train loss: 1.8880611658096313, validation loss: 0.9801998734474182, validation acc: 0.535\n",
      "Epoch: 0, 37.43%: train loss: 1.1796302795410156, validation loss: 0.9719364643096924, validation acc: 0.57\n",
      "Epoch: 0, 37.69%: train loss: 0.8523907661437988, validation loss: 0.9797675013542175, validation acc: 0.52\n",
      "Epoch: 0, 37.94%: train loss: 1.2371459007263184, validation loss: 1.0222641229629517, validation acc: 0.455\n",
      "Epoch: 0, 38.20%: train loss: 0.6744380593299866, validation loss: 1.002772569656372, validation acc: 0.525\n",
      "Epoch: 0, 38.45%: train loss: 1.2330224514007568, validation loss: 0.9752714037895203, validation acc: 0.49\n",
      "Epoch: 0, 38.71%: train loss: 0.49732106924057007, validation loss: 0.9964714646339417, validation acc: 0.5\n",
      "Epoch: 0, 38.96%: train loss: 1.2439864873886108, validation loss: 0.9440585374832153, validation acc: 0.6\n",
      "Epoch: 0, 39.22%: train loss: 1.334522008895874, validation loss: 1.0083683729171753, validation acc: 0.495\n",
      "Epoch: 0, 39.47%: train loss: 0.4905228018760681, validation loss: 0.9998630881309509, validation acc: 0.555\n",
      "Epoch: 0, 39.72%: train loss: 1.1631807088851929, validation loss: 0.9361322522163391, validation acc: 0.57\n",
      "Epoch: 0, 39.98%: train loss: 1.1972450017929077, validation loss: 0.9981290102005005, validation acc: 0.515\n",
      "Epoch: 0, 40.23%: train loss: 0.3328557014465332, validation loss: 0.9912118315696716, validation acc: 0.495\n",
      "Epoch: 0, 40.49%: train loss: 1.3778109550476074, validation loss: 0.984412431716919, validation acc: 0.53\n",
      "Epoch: 0, 40.74%: train loss: 1.0371530055999756, validation loss: 0.968453049659729, validation acc: 0.545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, 41.00%: train loss: 1.2633863687515259, validation loss: 0.9303086400032043, validation acc: 0.545\n",
      "Epoch: 0, 41.25%: train loss: 1.5489689111709595, validation loss: 0.9397305250167847, validation acc: 0.565\n",
      "Epoch: 0, 41.51%: train loss: 0.618009090423584, validation loss: 0.9751001000404358, validation acc: 0.57\n",
      "Epoch: 0, 41.76%: train loss: 0.2170577049255371, validation loss: 0.986003041267395, validation acc: 0.54\n",
      "Epoch: 0, 42.02%: train loss: 0.725407600402832, validation loss: 0.9795070290565491, validation acc: 0.555\n",
      "Epoch: 0, 42.27%: train loss: 1.3753286600112915, validation loss: 0.9913241267204285, validation acc: 0.5\n",
      "Epoch: 0, 42.53%: train loss: 0.9794507026672363, validation loss: 0.9792866110801697, validation acc: 0.505\n",
      "Epoch: 0, 42.78%: train loss: 0.7217432260513306, validation loss: 0.939351499080658, validation acc: 0.54\n",
      "Epoch: 0, 43.04%: train loss: 0.11345362663269043, validation loss: 0.9538116455078125, validation acc: 0.53\n",
      "Epoch: 0, 43.29%: train loss: 1.2457282543182373, validation loss: 0.9502466320991516, validation acc: 0.515\n",
      "Epoch: 0, 43.54%: train loss: 0.4395492672920227, validation loss: 0.9585551023483276, validation acc: 0.545\n",
      "Epoch: 0, 43.80%: train loss: 0.3198738098144531, validation loss: 0.9701881408691406, validation acc: 0.58\n",
      "Epoch: 0, 44.05%: train loss: 0.41186344623565674, validation loss: 1.0046801567077637, validation acc: 0.51\n",
      "Epoch: 0, 44.31%: train loss: 0.908819854259491, validation loss: 0.9662160277366638, validation acc: 0.535\n",
      "Epoch: 0, 44.56%: train loss: 0.7804043292999268, validation loss: 0.9805155992507935, validation acc: 0.535\n",
      "Epoch: 0, 44.82%: train loss: 0.849625289440155, validation loss: 0.9668152928352356, validation acc: 0.58\n",
      "Epoch: 0, 45.07%: train loss: 0.7907330393791199, validation loss: 1.0114609003067017, validation acc: 0.5\n",
      "Epoch: 0, 45.33%: train loss: 0.16462445259094238, validation loss: 0.9158657789230347, validation acc: 0.555\n",
      "Epoch: 0, 45.58%: train loss: 0.5725381374359131, validation loss: 0.990494430065155, validation acc: 0.53\n",
      "Epoch: 0, 45.84%: train loss: 0.3242284059524536, validation loss: 0.9022626280784607, validation acc: 0.57\n",
      "Epoch: 0, 46.09%: train loss: 0.27473580837249756, validation loss: 1.0053402185440063, validation acc: 0.555\n",
      "Epoch: 0, 46.35%: train loss: 0.585108757019043, validation loss: 0.9026144742965698, validation acc: 0.59\n",
      "Epoch: 0, 46.60%: train loss: 0.6540789604187012, validation loss: 0.9508398771286011, validation acc: 0.53\n",
      "Epoch: 0, 46.85%: train loss: 1.9718835353851318, validation loss: 0.9482215642929077, validation acc: 0.59\n",
      "Epoch: 0, 47.11%: train loss: 2.4625730514526367, validation loss: 0.928124725818634, validation acc: 0.54\n",
      "Epoch: 0, 47.36%: train loss: 0.5027148723602295, validation loss: 1.0433095693588257, validation acc: 0.455\n",
      "Epoch: 0, 47.62%: train loss: 1.1693214178085327, validation loss: 0.9758996963500977, validation acc: 0.52\n",
      "Epoch: 0, 47.87%: train loss: 1.2147414684295654, validation loss: 1.026149868965149, validation acc: 0.485\n",
      "Epoch: 0, 48.13%: train loss: 1.2314499616622925, validation loss: 0.9470188021659851, validation acc: 0.53\n",
      "Epoch: 0, 48.38%: train loss: 0.328460693359375, validation loss: 0.945716917514801, validation acc: 0.575\n",
      "Epoch: 0, 48.64%: train loss: 1.3035861253738403, validation loss: 1.0039230585098267, validation acc: 0.53\n",
      "Epoch: 0, 48.89%: train loss: 1.3168461322784424, validation loss: 0.9393701553344727, validation acc: 0.56\n",
      "Epoch: 0, 49.15%: train loss: 1.1316018104553223, validation loss: 1.0343525409698486, validation acc: 0.485\n",
      "Epoch: 0, 49.40%: train loss: 1.4932327270507812, validation loss: 0.9501751661300659, validation acc: 0.53\n",
      "Epoch: 0, 49.66%: train loss: 0.5828573107719421, validation loss: 0.953487753868103, validation acc: 0.53\n",
      "Epoch: 0, 49.91%: train loss: 1.070822834968567, validation loss: 0.9918211698532104, validation acc: 0.505\n",
      "Epoch: 0, 50.17%: train loss: 0.6532724499702454, validation loss: 0.9878482818603516, validation acc: 0.55\n",
      "Epoch: 0, 50.42%: train loss: 1.215078353881836, validation loss: 0.9088844060897827, validation acc: 0.56\n",
      "Epoch: 0, 50.67%: train loss: 0.3018709421157837, validation loss: 0.9722709655761719, validation acc: 0.5\n",
      "Epoch: 0, 50.93%: train loss: 1.4494552612304688, validation loss: 0.9313066601753235, validation acc: 0.56\n",
      "Epoch: 0, 51.18%: train loss: 0.8240774273872375, validation loss: 0.9121628403663635, validation acc: 0.56\n",
      "Epoch: 0, 51.44%: train loss: 1.0648373365402222, validation loss: 0.9869424104690552, validation acc: 0.51\n",
      "Epoch: 0, 51.69%: train loss: 0.5917807221412659, validation loss: 0.9489596486091614, validation acc: 0.575\n",
      "Epoch: 0, 51.95%: train loss: 0.5442788600921631, validation loss: 0.986735463142395, validation acc: 0.535\n",
      "Epoch: 0, 52.20%: train loss: 1.5847127437591553, validation loss: 0.9307218790054321, validation acc: 0.545\n",
      "Epoch: 0, 52.46%: train loss: 0.7017248272895813, validation loss: 0.9704809188842773, validation acc: 0.525\n",
      "Epoch: 0, 52.71%: train loss: 1.184521198272705, validation loss: 0.9287121891975403, validation acc: 0.54\n",
      "Epoch: 0, 52.97%: train loss: 1.6189974546432495, validation loss: 0.9690277576446533, validation acc: 0.51\n",
      "Epoch: 0, 53.22%: train loss: 2.6814749240875244, validation loss: 0.9523032903671265, validation acc: 0.51\n",
      "Epoch: 0, 53.48%: train loss: 1.493208885192871, validation loss: 0.9303079843521118, validation acc: 0.57\n",
      "Epoch: 0, 53.73%: train loss: 2.538914442062378, validation loss: 0.912202000617981, validation acc: 0.565\n",
      "Epoch: 0, 53.98%: train loss: 0.7790924310684204, validation loss: 0.9499684572219849, validation acc: 0.545\n",
      "Epoch: 0, 54.24%: train loss: 1.6057769060134888, validation loss: 0.9249172210693359, validation acc: 0.58\n",
      "Epoch: 0, 54.49%: train loss: 1.065129280090332, validation loss: 0.911240816116333, validation acc: 0.58\n",
      "Epoch: 0, 54.75%: train loss: 0.0018773078918457031, validation loss: 0.9418359994888306, validation acc: 0.57\n",
      "Epoch: 0, 55.00%: train loss: 0.13571596145629883, validation loss: 1.0125925540924072, validation acc: 0.46\n",
      "Epoch: 0, 55.26%: train loss: 0.34055662155151367, validation loss: 0.9784437417984009, validation acc: 0.54\n",
      "Epoch: 0, 55.51%: train loss: 1.759255051612854, validation loss: 1.0171856880187988, validation acc: 0.48\n",
      "Epoch: 0, 55.77%: train loss: 1.500288963317871, validation loss: 0.9550955891609192, validation acc: 0.535\n",
      "Epoch: 0, 56.02%: train loss: 0.6165741086006165, validation loss: 0.958674967288971, validation acc: 0.49\n",
      "Epoch: 0, 56.28%: train loss: 1.003180980682373, validation loss: 0.9880096912384033, validation acc: 0.51\n",
      "Epoch: 0, 56.53%: train loss: 0.2513614892959595, validation loss: 1.0212701559066772, validation acc: 0.5\n",
      "Epoch: 0, 56.79%: train loss: 1.2481619119644165, validation loss: 0.8996525406837463, validation acc: 0.595\n",
      "Epoch: 0, 57.04%: train loss: 1.2588297128677368, validation loss: 0.9250971078872681, validation acc: 0.57\n",
      "Epoch: 0, 57.30%: train loss: 0.4668554663658142, validation loss: 0.9523477554321289, validation acc: 0.58\n",
      "Epoch: 0, 57.55%: train loss: 1.671877145767212, validation loss: 0.9470561146736145, validation acc: 0.525\n",
      "Epoch: 0, 57.80%: train loss: 1.4554296731948853, validation loss: 0.9333133697509766, validation acc: 0.515\n",
      "Epoch: 0, 58.06%: train loss: 1.4133527278900146, validation loss: 0.9387347102165222, validation acc: 0.56\n",
      "Epoch: 0, 58.31%: train loss: 1.5450146198272705, validation loss: 0.9897624254226685, validation acc: 0.475\n",
      "Epoch: 0, 58.57%: train loss: 0.5949048399925232, validation loss: 0.960080087184906, validation acc: 0.55\n",
      "Epoch: 0, 58.82%: train loss: 1.213622808456421, validation loss: 0.9178048372268677, validation acc: 0.56\n",
      "Epoch: 0, 59.08%: train loss: 0.31819963455200195, validation loss: 0.9756001830101013, validation acc: 0.52\n",
      "Epoch: 0, 59.33%: train loss: 1.6166484355926514, validation loss: 0.9114187359809875, validation acc: 0.6\n",
      "Epoch: 0, 59.59%: train loss: 0.6647257804870605, validation loss: 0.9177539348602295, validation acc: 0.585\n",
      "Epoch: 0, 59.84%: train loss: 2.1657071113586426, validation loss: 1.0358161926269531, validation acc: 0.465\n",
      "Epoch: 0, 60.10%: train loss: 1.3384101390838623, validation loss: 1.0223652124404907, validation acc: 0.455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, 60.35%: train loss: 1.5986382961273193, validation loss: 0.9410046339035034, validation acc: 0.58\n",
      "Epoch: 0, 60.61%: train loss: 0.9610207080841064, validation loss: 0.9539223313331604, validation acc: 0.53\n",
      "Epoch: 0, 60.86%: train loss: 0.7043774724006653, validation loss: 0.931893527507782, validation acc: 0.57\n",
      "Epoch: 0, 61.12%: train loss: 0.8820306658744812, validation loss: 0.961658775806427, validation acc: 0.535\n",
      "Epoch: 0, 61.37%: train loss: 0.30208420753479004, validation loss: 0.9589882493019104, validation acc: 0.525\n",
      "Epoch: 0, 61.62%: train loss: 0.8126301169395447, validation loss: 0.9256336092948914, validation acc: 0.6\n",
      "Epoch: 0, 61.88%: train loss: 0.5087535977363586, validation loss: 0.915726363658905, validation acc: 0.53\n",
      "Epoch: 0, 62.13%: train loss: 1.0642166137695312, validation loss: 0.9307279586791992, validation acc: 0.56\n",
      "Epoch: 0, 62.39%: train loss: 0.6449586749076843, validation loss: 0.9802358150482178, validation acc: 0.505\n",
      "Epoch: 0, 62.64%: train loss: 0.9820263385772705, validation loss: 0.8919381499290466, validation acc: 0.565\n",
      "Epoch: 0, 62.90%: train loss: 0.8525025844573975, validation loss: 0.9779277443885803, validation acc: 0.49\n",
      "Epoch: 0, 63.15%: train loss: 1.521782398223877, validation loss: 0.8852338790893555, validation acc: 0.6\n",
      "Epoch: 0, 63.41%: train loss: 1.0265027284622192, validation loss: 0.8722448348999023, validation acc: 0.62\n",
      "Epoch: 0, 63.66%: train loss: 0.44969844818115234, validation loss: 0.9735516309738159, validation acc: 0.55\n",
      "Epoch: 0, 63.92%: train loss: 2.0230655670166016, validation loss: 0.9706307053565979, validation acc: 0.56\n",
      "Epoch: 0, 64.17%: train loss: 1.0457277297973633, validation loss: 0.9370588660240173, validation acc: 0.555\n",
      "Epoch: 0, 64.43%: train loss: 0.6724035143852234, validation loss: 0.9524844884872437, validation acc: 0.56\n",
      "Epoch: 0, 64.68%: train loss: 1.1157506704330444, validation loss: 0.9283661246299744, validation acc: 0.53\n",
      "Epoch: 0, 64.93%: train loss: 0.6663216948509216, validation loss: 0.9544097781181335, validation acc: 0.52\n",
      "Epoch: 0, 65.19%: train loss: 0.5721130967140198, validation loss: 0.9667707681655884, validation acc: 0.53\n",
      "Epoch: 0, 65.44%: train loss: 0.3344612121582031, validation loss: 1.0337191820144653, validation acc: 0.49\n",
      "Epoch: 0, 65.70%: train loss: 0.7514864206314087, validation loss: 0.9077414274215698, validation acc: 0.58\n",
      "Epoch: 0, 65.95%: train loss: 0.9753925204277039, validation loss: 0.9196171164512634, validation acc: 0.555\n",
      "Epoch: 0, 66.21%: train loss: 0.9155580401420593, validation loss: 1.032077670097351, validation acc: 0.48\n",
      "Epoch: 0, 66.46%: train loss: 1.0324395895004272, validation loss: 0.9353635907173157, validation acc: 0.54\n",
      "Epoch: 0, 66.72%: train loss: 0.5576193928718567, validation loss: 0.903141438961029, validation acc: 0.575\n",
      "Epoch: 0, 66.97%: train loss: 1.7245748043060303, validation loss: 1.0126363039016724, validation acc: 0.525\n",
      "Epoch: 0, 67.23%: train loss: 1.2431738376617432, validation loss: 0.9225554466247559, validation acc: 0.56\n",
      "Epoch: 0, 67.48%: train loss: 1.244284749031067, validation loss: 0.9849895238876343, validation acc: 0.485\n",
      "Epoch: 0, 67.74%: train loss: 0.5471373796463013, validation loss: 0.9587079286575317, validation acc: 0.525\n",
      "Epoch: 0, 67.99%: train loss: 0.5022540092468262, validation loss: 0.9172837734222412, validation acc: 0.585\n",
      "Epoch: 0, 68.25%: train loss: 0.34882354736328125, validation loss: 0.9559282660484314, validation acc: 0.56\n",
      "Epoch: 0, 68.50%: train loss: 0.3316560983657837, validation loss: 0.9363722801208496, validation acc: 0.555\n",
      "Epoch: 0, 68.75%: train loss: 0.7269817590713501, validation loss: 0.9189289808273315, validation acc: 0.61\n",
      "Epoch: 0, 69.01%: train loss: 1.3015525341033936, validation loss: 0.9390246272087097, validation acc: 0.53\n",
      "Epoch: 0, 69.26%: train loss: 0.25746357440948486, validation loss: 0.9691791534423828, validation acc: 0.54\n",
      "Epoch: 0, 69.52%: train loss: 0.9567021131515503, validation loss: 0.8971407413482666, validation acc: 0.58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-b9de72952182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < n_epochs:\n",
    "    model.train(True)\n",
    "    \n",
    "    for iteration, batch in enumerate(dataloader_train):\n",
    "        inputs, label = prep_inputs(batch)\n",
    "\n",
    "        predicted = model(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train = loss_func(predicted, label)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        iter_total = (epoch * len(dataloader_train)) + iteration\n",
    "        writer.add_scalar('train.loss', loss_train.data.cpu().numpy(), iter_total)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        if iteration > 0 and iteration % val_every == 0:\n",
    "#             compare_params(params_init, list(model.parameters()))\n",
    "            loss_val, acc_val = evaluate(model, dataloader_val, device=DEVICE, loss_func=loss_func, n_batches=200)\n",
    "\n",
    "            s = \"Epoch: {}, {:.2f}%: train loss: {}, validation loss: {}, validation acc: {}\".format(\n",
    "                epoch, (iteration / len(dataloader_train)) * 100, loss_train.data.cpu().numpy(), loss_val, acc_val\n",
    "            )\n",
    "            print(s)\n",
    "            writer.add_scalar('val.loss', loss_val, iter_total)\n",
    "            writer.add_scalar('val.acc', acc_val, iter_total)\n",
    "\n",
    "    print('\\n------------------------------------------------------------------------------------------------------------')\n",
    "    print(\"Epoch:\", epoch + 1, \"label accuracy:\", acc_val)\n",
    "    print('------------------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    torch.save(model.state_dict(), f=os.path.join(model_dir, '{}_{}_{}.pt'.format(model_str, epoch, iteration)))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_train.sentence1.apply(len).value_counts())\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_train.sentence2.apply(len).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
