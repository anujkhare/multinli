{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorboardX\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_utils, train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load train and val \n",
    "df_train = pd.read_pickle('data/df_train_train.pkl')\n",
    "df_val = pd.read_pickle('data/df_train_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_train = 'data/multinli_1.0_train.jsonl'\n",
    "file_path_heldout = 'data/multinli_1.0_dev_matched.jsonl'\n",
    "file_path_unlabeled = 'data/multinli_0.9_test_matched_unlabeled.jsonl'\n",
    "# df_train = data_utils.load_data(file_path_train)\n",
    "df_heldout = data_utils.load_data(file_path_heldout)\n",
    "df_unlabeled = data_utils.load_data(file_path_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled['gold_label'] = 'hidden'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pickle_file = 'weights/glove.pickle'\n",
    "if not os.path.exists(pickle_file):\n",
    "    glove = load_word_vectors('models/glove.840B.300d.txt')  # FIXME: There shold be 2196017 words\n",
    "    print(len(glove))\n",
    "\n",
    "    with open(pickle_file, 'wb') as outfile:\n",
    "        pickle.dump(glove, outfile)\n",
    "\n",
    "with open(pickle_file, 'rb') as infile:\n",
    "    glove = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=3\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_dataloader(df, sort_by_len: bool = True, shuffle: bool = False):\n",
    "    dataset = data_utils.MNLIDataset(df, word_vectors=glove, sort_by_len=sort_by_len)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=shuffle,\n",
    "                                            collate_fn=data_utils.collate_fn)\n",
    "    return dataset, dataloader\n",
    "\n",
    "dataset_train, dataloader_train = get_dataset_dataloader(df_train)\n",
    "dataset_val, dataloader_val = get_dataset_dataloader(df_val)\n",
    "dataset_heldout, dataloader_heldout = get_dataset_dataloader(df_heldout)\n",
    "dataset_unlabeled, dataloader_unlabeled = get_dataset_dataloader(df_unlabeled, sort_by_len=False, shuffle=False)\n",
    "\n",
    "print(len(dataset_train), len(dataloader_train))\n",
    "print(len(dataset_val), len(dataloader_val))\n",
    "print(len(dataset_heldout), len(dataloader_heldout))\n",
    "print(len(dataset_unlabeled), len(dataloader_unlabeled))  # NOTE: MAKE SURE THIS IS NOT SHUFFLED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(linear_size=512).cuda(device=DEVICE)\n",
    "\n",
    "loss_func = torch.nn.NLLLoss().cuda(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = 'lstm-batched-1'\n",
    "model_dir = '/opt/data/weights/{}'.format(model_str)\n",
    "log_dir = 'logs/{}'.format(model_str)\n",
    "\n",
    "os.makedirs(model_dir)\n",
    "writer = tensorboardX.SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_params = 0\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad: n_params += np.prod(param.size())\n",
    "\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_utils.train(model=model, dataloader_train=dataloader_train, dataloader_val=dataloader_val, optimizer=optimizer,\n",
    "                 loss_func=loss_func, model_dir=model_dir, n_epochs=8, device=DEVICE, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load('/opt/data/weights/lstm-2.4/lstm-2.4_2_392701.pt')\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses, accs = [], []\n",
    "for dataloader in [dataloader_train, dataloader_val, dataloader_heldout]:\n",
    "    loss, acc = train_utils.evaluate(model, dataloader, device=DEVICE, loss_func=loss_func, n_batches=1500)\n",
    "    print(loss, acc)\n",
    "    losses.append(loss)\n",
    "    accs.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = train_utils.predict(model, dataloader_unlabeled, device=DEVICE)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "df_unlabeled['gold_label'] = list(map(lambda x: data_utils.id_to_lbl[x], list(preds)))\n",
    "df_unlabeled[['pairID', 'gold_label']].to_csv('results/{}.csv'.format(model_str), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
